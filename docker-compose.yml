version: '1.0'

services:
 ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - 11434:11434
    networks:
      - chris_evil_llms
    healthcheck:
      test: ["CMD", "curl", "-f", "http://ollama:11434"]
      interval: 30s
      timeout: 10s
      retries: 3

 chatgpt:
    build:
      context: .
      dockerfile: Dockerfile.chatgpt
    ports:
      - 3000:3000
    environment:
      - 'DEFAULT_MODEL=llama2:latest'
      - 'OLLAMA_HOST=http://ollama:11434'
    depends_on:
      - ollama
    networks:
      - chris_evil_llms

 curl:
    image: curlimages/curl:latest
    command: >
      /bin/sh -c "
        for model in llama2 mistral; do
          if ! curl -s http://ollama:11434/api/tags | grep -wq $$model; then
            curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"'$$model'\"}' && sleep 5
          fi
        done
      "
    restart: on-failure
    depends_on:
      - ollama
    networks:
      - chris_evil_llms


volumes:
 ollama:

networks:
 chris_evil_llms:
    driver: bridge
